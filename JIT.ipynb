{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The PyTorch JIT\n",
    "## PyCon 2019 - Berlin\n",
    "### Tilman Krokotsch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About Me\n",
    "\n",
    "### Tilman Krokotsch\n",
    "#### Deep Learning Engineer @ IAV GmbH automotive engineering\n",
    "#### PhD Student @ TU Berlin under Prof. Clemens GÃ¼hmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First of all, we have to explain what a JIT is. The Acronym stands for Just-In-Time compiler. It compiles source code to machine code during runtime. This way the code can be serialized for easy transfer and optimized for faster execution. In our case, the JIT can compile a subset of the Python language, called TorchScript, to a computation graph. This graph can then be executed just as you would with a TensorFlow graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are we talking about?\n",
    "\n",
    "### JIT == Just-in-time compiler\n",
    "\n",
    "> \\[..\\] is a way to create serializable and optimizable models from PyTorch code. _**- PyTorch Docs**_\n",
    "\n",
    "### Examples\n",
    "* Java Virtual Machine (JVM)\n",
    "* PHP 8 in 2021 \\[[Source](https://hub.packtpub.com/php-8-and-7-4-to-come-with-just-in-time-jit-to-make-most-cpu-intensive-workloads-run-significantly-faster/)\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "These are the promises I made in the title of this talk. They are based on my understanding of the JIT at the time of writing the talk proposal. Since then a lot happened (e.g. a minor PyTorch release), so let's see how these promises hold up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Promises\n",
    "\n",
    "### 1. Minimize Dependencies\n",
    "### 2. Hide Code\n",
    "### 3. Boost Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imports and Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First of all we need imports for PyTorch itself, torchvision for its pretrained models and the JIT module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.jit as jit\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import f_oneway\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We will use a pretrained AlexNet from the torchvision model zoo for this example. Let's load it and have a look at its architecture. Printing the network lets us know pretty much everything: layer types, layer order, kernel sizes and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = torchvision.models.alexnet(pretrained=True)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are we competing against?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "There are two ways to conventionally save a PyTorch model. The first one just calls the save function on the model itself. The second one calls save on the state_dict of the model. The state_dict is a dictionary of all parameters and buffers of the model. Both methods use the pickle module internally and while the first pickles the entire model object, the second pickles the state_dict. The Docs recommend the second method. We will see later, why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(net, 'untraced_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'state_dict_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "An odder way to save your model, is to export it to ONNX. The Open Neural Network eXchange format is supported by some frameworks (e.g. CNTK and Caffe2) and is aimed at making transfer of models between them more accessible. To export our model, we need to record the computational graph with an examplary input. The graph and the weights are then saved into a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 244, 244)\n",
    "torch.onnx.export(net, x, \"onnx_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to use the JIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now we can convert our network into TorchScript, the language used by the JIT. For most feed forward networks this is done by tracing. We set the network into evaluation mode, as we want to deploy it, and define a representative input. The trace() function feeds the input through the forward() function of our network and records all operations. Out comes our desired ScriptedModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TracedModule[AlexNet](\n",
      "  (features): TracedModule[Sequential](\n",
      "    (0): TracedModule[Conv2d]()\n",
      "    (1): TracedModule[ReLU]()\n",
      "    (2): TracedModule[MaxPool2d]()\n",
      "    (3): TracedModule[Conv2d]()\n",
      "    (4): TracedModule[ReLU]()\n",
      "    (5): TracedModule[MaxPool2d]()\n",
      "    (6): TracedModule[Conv2d]()\n",
      "    (7): TracedModule[ReLU]()\n",
      "    (8): TracedModule[Conv2d]()\n",
      "    (9): TracedModule[ReLU]()\n",
      "    (10): TracedModule[Conv2d]()\n",
      "    (11): TracedModule[ReLU]()\n",
      "    (12): TracedModule[MaxPool2d]()\n",
      "  )\n",
      "  (avgpool): TracedModule[AdaptiveAvgPool2d]()\n",
      "  (classifier): TracedModule[Sequential](\n",
      "    (0): TracedModule[Dropout]()\n",
      "    (1): TracedModule[Linear]()\n",
      "    (2): TracedModule[ReLU]()\n",
      "    (3): TracedModule[Dropout]()\n",
      "    (4): TracedModule[Linear]()\n",
      "    (5): TracedModule[ReLU]()\n",
      "    (6): TracedModule[Linear]()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 244, 244)\n",
    "net.eval()\n",
    "traced_net = jit.trace(net, x)\n",
    "print(traced_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our traced network is now ready to be written to disk. For that we use the save() function of the jit module. It works the same as the conentional torch.save() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "jit.save(traced_net, 'traced_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# That's it folks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We have saved our network. Now let's head over to a fresh notebook where we can load and test it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Boost Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let us have a look at the runtime of our models. First we will run our untraced network and then the one traced by the JIT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749.774 ms mean with 14.736 std for 10 runs with 10 loops each\n"
     ]
    }
   ],
   "source": [
    "untraced_runs = []\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "for _ in range(10):\n",
    "    with torch.autograd.profiler.profile() as profile:\n",
    "        for _ in range(10):\n",
    "            net(x)\n",
    "    untraced_runs.append(profile.self_cpu_time_total / 1000)\n",
    "print('%.3f ms mean with %.3f std for 10 runs with 10 loops each' % (np.mean(runs), np.std(runs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749.774 ms mean with 14.736 std for 10 runs with 10 loops each\n"
     ]
    }
   ],
   "source": [
    "traced_runs = []\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "for _ in range(10):\n",
    "    with torch.autograd.profiler.profile() as profile:\n",
    "        for _ in range(10):\n",
    "            traced_net(x)\n",
    "    traced_runs.append(profile.self_cpu_time_total / 1000)\n",
    "print('%.3f ms mean with %.3f std for 10 runs with 10 loops each' % (np.mean(runs), np.std(runs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It seems that our traced network is slightly faster. An ANOVA on the runs can tell us if the difference is statistically significant. We get a p-value grater than 0.05, far away from any significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F_onewayResult(statistic=2.9917420848412974, pvalue=0.10080043993069403)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_oneway(untraced_runs, traced_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "But this was only on the CPU. What about performance on GPU? As we have no GPU available here, we will import the timeit runs from a machine equiped with a GTX 1080Ti. This time the ANOVA shows a p-value of 9e-9, which is by significant by all accounts. Unfortunatelly the traced network is only 1.5 ms faster on average, which is a quite small improvement. For better illustration: the untraced network can process 53 images per second, while the traced can do 58."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F_onewayResult(statistic=99.06722724800497, pvalue=9.598289543000189e-09)\n",
      "Untraced: 18.70643539428711\n",
      "Traced: 17.2982816696167\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('torchvision_timings.pkl', mode='rb') as f:\n",
    "    time_dict = pickle.load(f)\n",
    "print(f_oneway(time_dict['alexnet']['untraced'], time_dict['alexnet']['traced']))\n",
    "print('Untraced:', np.mean(time_dict['alexnet']['untraced']))\n",
    "print('Traced:', np.mean(time_dict['alexnet']['traced']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The last promise is theoretically fulfilled for this network. No significant speedup is meassurable for our simple network on CPU and a small one on GPU. We will look at some advanced uses of the JIT next and see if our findings hold up there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Performance of TorchVision Networks\n",
    "\n",
    "![Runtime comparisons of Torchvision networks](./torchvision_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For the relatively simple AlexNet we now know what we can expect from tracing it. What about more complex networks? For this we will repeat the process above for each network in torchvision. The JIT will inform us, that the outputs of some traced networks do not correspond with the untraced ones. As we are using untrained versions of the networks, we will ignore this warning for now. Again we will have a look at the results from a machine with a GPU.\n",
    "\n",
    "As we can see, there is next to no difference in mean execution time for all networks. While most networks report significant differences in execution time, the absolute difference is, again, magnitudes smaller than execution time itself. The googlenet architecture alone seems faster when traced. Looking at the source code I could not make out any differences in this network, compared to the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using the Model in C++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To load our traced model into a C++ application, we will use the code below. It is a minimal, compilable example of what we want to do. We read the path to the model from the commandline arguments and instanciate a new _jit::script::Module_. Then we load it with the _jit::load_ function, which is Ã¤quivalent to the Python one. Any error while loading is caught."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#include <torch/script.h>\n",
    "#include <memory>\n",
    "\n",
    "int main(int argc, const char* argv[]) {\n",
    "  if (argc != 2) {\n",
    "    std::cerr << \"usage: example-app <path-to-exported-script-module>\\n\";\n",
    "    return -1;\n",
    "  }\n",
    "\n",
    "\n",
    "  torch::jit::script::Module module;\n",
    "  try {\n",
    "    module = torch::jit::load(argv[1]);\n",
    "  }\n",
    "  catch (const c10::Error& e) {\n",
    "    std::cerr << \"error loading the model\\n\";\n",
    "    return -1;\n",
    "  }\n",
    "\n",
    "  std::cout << \"ok\\n\";\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Using the forward pass of our model is similar to Python, too. We build an input vector of tensors and pass it to the network. The output is saved to a new tensor object. Different to Python is, that the tensor class lives in the ATen namespace, _at_. Furthermore the module does not accept and output tensors directly. Instead it uses _jit::IValue_ objects. The input tensor is automatically cast to an _IValue_ on assignment to the input vector and the module output is explicitly cast by the _toTensor_ function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "std::vector<torch::jit::IValue> inputs;\n",
    "inputs.push_back(torch::randn({1, 3, 224, 224}));\n",
    "at::Tensor output;\n",
    "\n",
    "output = module.forward(inputs).toTensor();\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Design Choices for Migrating Your Model Code\n",
    "\n",
    "* Migrate step-by-step\n",
    "\n",
    "* Avoid non-traceable code when possible\n",
    "\n",
    "* Think about your interface"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
