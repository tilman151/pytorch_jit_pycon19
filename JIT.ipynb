{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The PyTorch JIT\n",
    "## PyCon 2019 - Berlin\n",
    "### Tilman Krokotsch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About Me\n",
    "\n",
    "### Tilman Krokotsch\n",
    "#### Deep Learning Engineer @ IAV GmbH automotive engineering\n",
    "#### PhD Student @ TU Berlin under Prof. Clemens GÃ¼hmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imports and Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First of all we need imports for PyTorch itself, torchvision for the pretrained models, numpy for feeding PyTorch, pillow for image manipulation and json for loading JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.jit as jit\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import json\n",
    "\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with open('./imagenet_classes.json', mode='rt') as f:\n",
    "    CLASS_DICT = json.load(f)\n",
    "    \n",
    "IMAGENET_MEAN = np.array([[[0.485, 0.456, 0.406]]], dtype=np.float32)\n",
    "IMAGENET_STD = np.array([[[0.229, 0.224, 0.225]]], dtype=np.float32)\n",
    "\n",
    "def predict_imagenet(net, image_file):\n",
    "    # Load image and resize\n",
    "    image = PIL.Image.open(image_file).resize((244, 244))\n",
    "    # Convert to numpy and normalize\n",
    "    image = np.array(image, dtype=np.float32) / 255.\n",
    "    image = (image - IMAGENET_MEAN) / IMAGENET_STD\n",
    "    # Convert to PyTorch and make channel first\n",
    "    image = torch.as_tensor(image).unsqueeze(0).permute(0, 3, 1, 2)\n",
    "    # Predict top class\n",
    "    logits = net(image)\n",
    "    class_idx = logits.squeeze(0).argmax().item()\n",
    "    # Output predictions\n",
    "    print('It is a %s.' % CLASS_DICT[str(class_idx)])\n",
    "    display(Image(filename=image_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We will use a pretrained AlexNet from the torchvision model zoo for this example. Let's load it and have a look at its architecture. Printing the network lets us know pretty much everything: layer types, layer order, kernel sizes and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = torchvision.models.alexnet(pretrained=True)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now we can convert our network into TorchScript, the language used by the JIT. For most feed forward networks this is done by tracing. We set the network into evaluation mode, as we want to deploy it, and define a representative input. The trace() function feeds the input through the forward() function of our network and records all operations. Out comes our desired ScriptedModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TracedModule[AlexNet](\n",
      "  (features): TracedModule[Sequential](\n",
      "    (0): TracedModule[Conv2d]()\n",
      "    (1): TracedModule[ReLU]()\n",
      "    (2): TracedModule[MaxPool2d]()\n",
      "    (3): TracedModule[Conv2d]()\n",
      "    (4): TracedModule[ReLU]()\n",
      "    (5): TracedModule[MaxPool2d]()\n",
      "    (6): TracedModule[Conv2d]()\n",
      "    (7): TracedModule[ReLU]()\n",
      "    (8): TracedModule[Conv2d]()\n",
      "    (9): TracedModule[ReLU]()\n",
      "    (10): TracedModule[Conv2d]()\n",
      "    (11): TracedModule[ReLU]()\n",
      "    (12): TracedModule[MaxPool2d]()\n",
      "  )\n",
      "  (avgpool): TracedModule[AdaptiveAvgPool2d]()\n",
      "  (classifier): TracedModule[Sequential](\n",
      "    (0): TracedModule[Dropout]()\n",
      "    (1): TracedModule[Linear]()\n",
      "    (2): TracedModule[ReLU]()\n",
      "    (3): TracedModule[Dropout]()\n",
      "    (4): TracedModule[Linear]()\n",
      "    (5): TracedModule[ReLU]()\n",
      "    (6): TracedModule[Linear]()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 244, 244)\n",
    "net.eval()\n",
    "traced_net = jit.trace(net, x)\n",
    "print(traced_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our traced network is now ready to be written to disk. For that we use the save() function of the jit module. It works the same as the conentional torch.save() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "jit.save(traced_net, './model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# That's it folks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We have saved our network. Now let's head over to a fresh notebook where we can load and test it out."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
