{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The PyTorch JIT\n",
    "## PyCon 2019 - Berlin\n",
    "### Tilman Krokotsch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# About Me\n",
    "\n",
    "### Tilman Krokotsch\n",
    "#### Deep Learning Engineer @ IAV Digital Lab\n",
    "#### PhD Student @ TU Berlin under Prof. Clemens GÃ¼hmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First of all, we have to explain what a JIT is. The Acronym stands for Just-In-Time compiler. It compiles source code to machine code during runtime. This way the code can be serialized for easy transfer and optimized for faster execution. In our case, the JIT can compile the forward pass of a neural network from Python to a computation graph representation. This graph can then be executed directly in the C++ runtine of PyTorch without intermediate callbacks to Python. The graph representations has several advantages for deploying neural network models, which we will investigate in this talk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are we talking about?\n",
    "\n",
    "### JIT == Just-in-time compiler\n",
    "\n",
    "> \\[..\\] is a way to create serializable and optimizable models from PyTorch code. _**- PyTorch Docs**_\n",
    "\n",
    "\n",
    "### Compiles NN forward pass from Python into computation graph\n",
    "\n",
    "### Executes computation graph directly in the C++ runtime of PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# What do we need it for?\n",
    "\n",
    "### Deployment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "These are the promises I made in the title of this talk. They are based on my understanding of the JIT at the time of writing the talk proposal. Since then a lot happened (e.g. a minor PyTorch release), so let's see how these promises hold up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Promises\n",
    "\n",
    "### 1. Minimize Dependencies\n",
    "### 2. Hide Code\n",
    "### 3. Boost Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imports and Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "First of all we need imports for PyTorch itself, torchvision for its pretrained models and the JIT module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.jit as jit\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.stats import f_oneway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We will use a pretrained AlexNet from the torchvision model zoo for this example. Let's load it and have a look at its architecture. Printing the network lets us know pretty much everything: layer types, layer order, kernel sizes and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = torchvision.models.alexnet(pretrained=True)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are we competing against?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "There are two ways to conventionally save a PyTorch model. The first one just calls the save function on the model itself. The second one calls save on the state_dict of the model. The state_dict is a dictionary of all parameters and buffers of the model. Both methods use the pickle module internally and while the first pickles the entire model object, the second pickles the state_dict. The Docs recommend the second method. We will see later, why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Save the whole module object to a pickle file\n",
    "torch.save(net, 'untraced_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Save only the weights and buffers to a pickle file\n",
    "torch.save(net.state_dict(), 'state_dict_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "An odder way to save your model, is to export it to ONNX. The Open Neural Network eXchange format is supported by some frameworks (e.g. CNTK and Caffe2) and is aimed at making transfer of models between them more accessible. To export our model, we need to record the computational graph with an examplary input. The graph and the weights are then saved into a single file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Build an ONNX graph and save the module to an ONNX file\n",
    "x = torch.randn(1, 3, 244, 244)\n",
    "net.eval()\n",
    "torch.onnx.export(net, x, \"onnx_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How to use the JIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Now we can convert our network into TorchScript, the language used by the JIT. For most feed forward networks this is done by tracing. We set the network into evaluation mode, as we want to deploy it, and define a representative input. The trace() function feeds the input through the forward() function of our network and records all operations. Out comes our desired ScriptedModule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%self : ClassType<AlexNet>,\n",
      "      %input.1 : Float(1, 3, 244, 244)):\n",
      "  %1 : ClassType<Sequential> = prim::GetAttr[name=\"features\"](%self)\n",
      "  %2 : ClassType<Conv2d> = prim::GetAttr[name=\"0\"](%1)\n",
      "  %weight.1 : Tensor = prim::GetAttr[name=\"weight\"](%2)\n",
      "  %4 : Tensor = prim::GetAttr[name=\"bias\"](%2)\n",
      "  %7 : ClassType<Conv2d> = prim::GetAttr[name=\"3\"](%1)\n",
      "  %weight.2 : Tensor = prim::GetAttr[name=\"weight\"](%7)\n",
      "  %9 : Tensor = prim::GetAttr[name=\"bias\"](%7)\n",
      "  %12 : ClassType<Conv2d> = prim::GetAttr[name=\"6\"](%1)\n",
      "  %weight.3 : Tensor = prim::GetAttr[name=\"weight\"](%12)\n",
      "  %14 : Tensor = prim::GetAttr[name=\"bias\"](%12)\n",
      "  %16 : ClassType<Conv2d> = prim::GetAttr[name=\"8\"](%1)\n",
      "  %weight.4 : Tensor = prim::GetAttr[name=\"weight\"](%16)\n",
      "  %18 : Tensor = prim::GetAttr[name=\"bias\"](%16)\n",
      "  %20 : ClassType<Conv2d> = prim::GetAttr[name=\"10\"](%1)\n",
      "  %weight.5 : Tensor = prim::GetAttr[name=\"weight\"](%20)\n",
      "  %22 : Tensor = prim::GetAttr[name=\"bias\"](%20)\n",
      "  %26 : ClassType<Sequential> = prim::GetAttr[name=\"classifier\"](%self)\n",
      "  %28 : ClassType<Linear> = prim::GetAttr[name=\"1\"](%26)\n",
      "  %weight.6 : Tensor = prim::GetAttr[name=\"weight\"](%28)\n",
      "  %bias.1 : Tensor = prim::GetAttr[name=\"bias\"](%28)\n",
      "  %33 : ClassType<Linear> = prim::GetAttr[name=\"4\"](%26)\n",
      "  %weight.7 : Tensor = prim::GetAttr[name=\"weight\"](%33)\n",
      "  %bias.2 : Tensor = prim::GetAttr[name=\"bias\"](%33)\n",
      "  %37 : ClassType<Linear> = prim::GetAttr[name=\"6\"](%26)\n",
      "  %weight : Tensor = prim::GetAttr[name=\"weight\"](%37)\n",
      "  %bias : Tensor = prim::GetAttr[name=\"bias\"](%37)\n",
      "  %41 : int = prim::Constant[value=4](), scope: AlexNet/Sequential[features]/Conv2d[0] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %42 : int = prim::Constant[value=4](), scope: AlexNet/Sequential[features]/Conv2d[0] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %43 : int[] = prim::ListConstruct(%41, %42), scope: AlexNet/Sequential[features]/Conv2d[0]\n",
      "  %44 : int = prim::Constant[value=2](), scope: AlexNet/Sequential[features]/Conv2d[0] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %45 : int = prim::Constant[value=2](), scope: AlexNet/Sequential[features]/Conv2d[0] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %46 : int[] = prim::ListConstruct(%44, %45), scope: AlexNet/Sequential[features]/Conv2d[0]\n",
      "  %47 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[0] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %48 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[0] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %49 : int[] = prim::ListConstruct(%47, %48), scope: AlexNet/Sequential[features]/Conv2d[0]\n",
      "  %50 : bool = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[0] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %51 : int = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[0] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %52 : int = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[0] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %53 : int[] = prim::ListConstruct(%51, %52), scope: AlexNet/Sequential[features]/Conv2d[0]\n",
      "  %54 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[0] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %55 : bool = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[0] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %56 : bool = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[0] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %57 : bool = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[0] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %input.2 : Float(1, 64, 60, 60) = aten::_convolution(%input.1, %weight.1, %4, %43, %46, %49, %50, %53, %54, %55, %56, %57), scope: AlexNet/Sequential[features]/Conv2d[0] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %input.3 : Float(1, 64, 60, 60) = aten::relu_(%input.2), scope: AlexNet/Sequential[features]/ReLU[1] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:911:0\n",
      "  %60 : int = prim::Constant[value=3](), scope: AlexNet/Sequential[features]/MaxPool2d[2] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %61 : int = prim::Constant[value=3](), scope: AlexNet/Sequential[features]/MaxPool2d[2] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %62 : int[] = prim::ListConstruct(%60, %61), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n",
      "  %63 : int = prim::Constant[value=2](), scope: AlexNet/Sequential[features]/MaxPool2d[2] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %64 : int = prim::Constant[value=2](), scope: AlexNet/Sequential[features]/MaxPool2d[2] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %65 : int[] = prim::ListConstruct(%63, %64), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n",
      "  %66 : int = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/MaxPool2d[2] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %67 : int = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/MaxPool2d[2] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %68 : int[] = prim::ListConstruct(%66, %67), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n",
      "  %69 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/MaxPool2d[2] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %70 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/MaxPool2d[2] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %71 : int[] = prim::ListConstruct(%69, %70), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n",
      "  %72 : bool = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/MaxPool2d[2] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %input.4 : Float(1, 64, 29, 29) = aten::max_pool2d(%input.3, %62, %65, %68, %71, %72), scope: AlexNet/Sequential[features]/MaxPool2d[2] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %74 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[3] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %75 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[3] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %76 : int[] = prim::ListConstruct(%74, %75), scope: AlexNet/Sequential[features]/Conv2d[3]\n",
      "  %77 : int = prim::Constant[value=2](), scope: AlexNet/Sequential[features]/Conv2d[3] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %78 : int = prim::Constant[value=2](), scope: AlexNet/Sequential[features]/Conv2d[3] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %79 : int[] = prim::ListConstruct(%77, %78), scope: AlexNet/Sequential[features]/Conv2d[3]\n",
      "  %80 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[3] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %81 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[3] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %82 : int[] = prim::ListConstruct(%80, %81), scope: AlexNet/Sequential[features]/Conv2d[3]\n",
      "  %83 : bool = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[3] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %84 : int = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[3] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %85 : int = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[3] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %86 : int[] = prim::ListConstruct(%84, %85), scope: AlexNet/Sequential[features]/Conv2d[3]\n",
      "  %87 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[3] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %88 : bool = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[3] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %89 : bool = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[3] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %90 : bool = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[3] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %input.5 : Float(1, 192, 29, 29) = aten::_convolution(%input.4, %weight.2, %9, %76, %79, %82, %83, %86, %87, %88, %89, %90), scope: AlexNet/Sequential[features]/Conv2d[3] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %input.6 : Float(1, 192, 29, 29) = aten::relu_(%input.5), scope: AlexNet/Sequential[features]/ReLU[4] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:911:0\n",
      "  %93 : int = prim::Constant[value=3](), scope: AlexNet/Sequential[features]/MaxPool2d[5] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %94 : int = prim::Constant[value=3](), scope: AlexNet/Sequential[features]/MaxPool2d[5] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %95 : int[] = prim::ListConstruct(%93, %94), scope: AlexNet/Sequential[features]/MaxPool2d[5]\n",
      "  %96 : int = prim::Constant[value=2](), scope: AlexNet/Sequential[features]/MaxPool2d[5] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %97 : int = prim::Constant[value=2](), scope: AlexNet/Sequential[features]/MaxPool2d[5] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %98 : int[] = prim::ListConstruct(%96, %97), scope: AlexNet/Sequential[features]/MaxPool2d[5]\n",
      "  %99 : int = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/MaxPool2d[5] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %100 : int = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/MaxPool2d[5] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %101 : int[] = prim::ListConstruct(%99, %100), scope: AlexNet/Sequential[features]/MaxPool2d[5]\n",
      "  %102 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/MaxPool2d[5] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %103 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/MaxPool2d[5] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %104 : int[] = prim::ListConstruct(%102, %103), scope: AlexNet/Sequential[features]/MaxPool2d[5]\n",
      "  %105 : bool = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/MaxPool2d[5] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %input.7 : Float(1, 192, 14, 14) = aten::max_pool2d(%input.6, %95, %98, %101, %104, %105), scope: AlexNet/Sequential[features]/MaxPool2d[5] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %107 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[6] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %108 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[6] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %109 : int[] = prim::ListConstruct(%107, %108), scope: AlexNet/Sequential[features]/Conv2d[6]\n",
      "  %110 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[6] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %111 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[6] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %112 : int[] = prim::ListConstruct(%110, %111), scope: AlexNet/Sequential[features]/Conv2d[6]\n",
      "  %113 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[6] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %114 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[6] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %115 : int[] = prim::ListConstruct(%113, %114), scope: AlexNet/Sequential[features]/Conv2d[6]\n",
      "  %116 : bool = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[6] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %117 : int = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[6] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %118 : int = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[6] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %119 : int[] = prim::ListConstruct(%117, %118), scope: AlexNet/Sequential[features]/Conv2d[6]\n",
      "  %120 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[6] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %121 : bool = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[6] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %122 : bool = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[6] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %123 : bool = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[6] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %input.8 : Float(1, 384, 14, 14) = aten::_convolution(%input.7, %weight.3, %14, %109, %112, %115, %116, %119, %120, %121, %122, %123), scope: AlexNet/Sequential[features]/Conv2d[6] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %input.9 : Float(1, 384, 14, 14) = aten::relu_(%input.8), scope: AlexNet/Sequential[features]/ReLU[7] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:911:0\n",
      "  %126 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[8] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %127 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[8] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %128 : int[] = prim::ListConstruct(%126, %127), scope: AlexNet/Sequential[features]/Conv2d[8]\n",
      "  %129 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[8] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %130 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[8] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %131 : int[] = prim::ListConstruct(%129, %130), scope: AlexNet/Sequential[features]/Conv2d[8]\n",
      "  %132 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[8] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %133 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[8] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %134 : int[] = prim::ListConstruct(%132, %133), scope: AlexNet/Sequential[features]/Conv2d[8]\n",
      "  %135 : bool = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[8] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %136 : int = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[8] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %137 : int = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[8] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %138 : int[] = prim::ListConstruct(%136, %137), scope: AlexNet/Sequential[features]/Conv2d[8]\n",
      "  %139 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[8] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %140 : bool = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[8] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %141 : bool = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[8] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %142 : bool = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[8] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %input.10 : Float(1, 256, 14, 14) = aten::_convolution(%input.9, %weight.4, %18, %128, %131, %134, %135, %138, %139, %140, %141, %142), scope: AlexNet/Sequential[features]/Conv2d[8] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %input.11 : Float(1, 256, 14, 14) = aten::relu_(%input.10), scope: AlexNet/Sequential[features]/ReLU[9] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:911:0\n",
      "  %145 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[10] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %146 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[10] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %147 : int[] = prim::ListConstruct(%145, %146), scope: AlexNet/Sequential[features]/Conv2d[10]\n",
      "  %148 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[10] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %149 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[10] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %150 : int[] = prim::ListConstruct(%148, %149), scope: AlexNet/Sequential[features]/Conv2d[10]\n",
      "  %151 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[10] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %152 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[10] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %153 : int[] = prim::ListConstruct(%151, %152), scope: AlexNet/Sequential[features]/Conv2d[10]\n",
      "  %154 : bool = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[10] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %155 : int = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[10] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %156 : int = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[10] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %157 : int[] = prim::ListConstruct(%155, %156), scope: AlexNet/Sequential[features]/Conv2d[10]\n",
      "  %158 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[10] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %159 : bool = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[10] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %160 : bool = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/Conv2d[10] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %161 : bool = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/Conv2d[10] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %input.12 : Float(1, 256, 14, 14) = aten::_convolution(%input.11, %weight.5, %22, %147, %150, %153, %154, %157, %158, %159, %160, %161), scope: AlexNet/Sequential[features]/Conv2d[10] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:340:0\n",
      "  %input.13 : Float(1, 256, 14, 14) = aten::relu_(%input.12), scope: AlexNet/Sequential[features]/ReLU[11] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:911:0\n",
      "  %164 : int = prim::Constant[value=3](), scope: AlexNet/Sequential[features]/MaxPool2d[12] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %165 : int = prim::Constant[value=3](), scope: AlexNet/Sequential[features]/MaxPool2d[12] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %166 : int[] = prim::ListConstruct(%164, %165), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n",
      "  %167 : int = prim::Constant[value=2](), scope: AlexNet/Sequential[features]/MaxPool2d[12] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %168 : int = prim::Constant[value=2](), scope: AlexNet/Sequential[features]/MaxPool2d[12] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %169 : int[] = prim::ListConstruct(%167, %168), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n",
      "  %170 : int = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/MaxPool2d[12] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %171 : int = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/MaxPool2d[12] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %172 : int[] = prim::ListConstruct(%170, %171), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n",
      "  %173 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/MaxPool2d[12] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %174 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[features]/MaxPool2d[12] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %175 : int[] = prim::ListConstruct(%173, %174), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n",
      "  %176 : bool = prim::Constant[value=0](), scope: AlexNet/Sequential[features]/MaxPool2d[12] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %input.14 : Float(1, 256, 6, 6) = aten::max_pool2d(%input.13, %166, %169, %172, %175, %176), scope: AlexNet/Sequential[features]/MaxPool2d[12] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:487:0\n",
      "  %190 : int = prim::Constant[value=6](), scope: AlexNet/AdaptiveAvgPool2d[avgpool] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:767:0\n",
      "  %191 : int = prim::Constant[value=6](), scope: AlexNet/AdaptiveAvgPool2d[avgpool] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:767:0\n",
      "  %192 : int[] = prim::ListConstruct(%190, %191), scope: AlexNet/AdaptiveAvgPool2d[avgpool]\n",
      "  %x : Float(1, 256, 6, 6) = aten::adaptive_avg_pool2d(%input.14, %192), scope: AlexNet/AdaptiveAvgPool2d[avgpool] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:767:0\n",
      "  %194 : int = prim::Constant[value=1](), scope: AlexNet # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torchvision\\models\\alexnet.py:47:0\n",
      "  %195 : int = prim::Constant[value=-1](), scope: AlexNet # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torchvision\\models\\alexnet.py:47:0\n",
      "  %input.15 : Float(1, 9216) = aten::flatten(%x, %194, %195), scope: AlexNet # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torchvision\\models\\alexnet.py:47:0\n",
      "  %197 : float = prim::Constant[value=0.5](), scope: AlexNet/Sequential[classifier]/Dropout[0] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:806:0\n",
      "  %198 : bool = prim::Constant[value=0](), scope: AlexNet/Sequential[classifier]/Dropout[0] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:806:0\n",
      "  %input.16 : Float(1, 9216) = aten::dropout(%input.15, %197, %198), scope: AlexNet/Sequential[classifier]/Dropout[0] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:806:0\n",
      "  %200 : Float(9216!, 4096!) = aten::t(%weight.6), scope: AlexNet/Sequential[classifier]/Linear[1] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:1369:0\n",
      "  %201 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[classifier]/Linear[1] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:1369:0\n",
      "  %202 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[classifier]/Linear[1] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:1369:0\n",
      "  %input.17 : Float(1, 4096) = aten::addmm(%bias.1, %input.16, %200, %201, %202), scope: AlexNet/Sequential[classifier]/Linear[1] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:1369:0\n",
      "  %input.18 : Float(1, 4096) = aten::relu_(%input.17), scope: AlexNet/Sequential[classifier]/ReLU[2] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:911:0\n",
      "  %205 : float = prim::Constant[value=0.5](), scope: AlexNet/Sequential[classifier]/Dropout[3] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:806:0\n",
      "  %206 : bool = prim::Constant[value=0](), scope: AlexNet/Sequential[classifier]/Dropout[3] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:806:0\n",
      "  %input.19 : Float(1, 4096) = aten::dropout(%input.18, %205, %206), scope: AlexNet/Sequential[classifier]/Dropout[3] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:806:0\n",
      "  %208 : Float(4096!, 4096!) = aten::t(%weight.7), scope: AlexNet/Sequential[classifier]/Linear[4] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:1369:0\n",
      "  %209 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[classifier]/Linear[4] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:1369:0\n",
      "  %210 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[classifier]/Linear[4] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:1369:0\n",
      "  %input.20 : Float(1, 4096) = aten::addmm(%bias.2, %input.19, %208, %209, %210), scope: AlexNet/Sequential[classifier]/Linear[4] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:1369:0\n",
      "  %input : Float(1, 4096) = aten::relu_(%input.20), scope: AlexNet/Sequential[classifier]/ReLU[5] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:911:0\n",
      "  %213 : Float(4096!, 1000!) = aten::t(%weight), scope: AlexNet/Sequential[classifier]/Linear[6] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:1369:0\n",
      "  %214 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[classifier]/Linear[6] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:1369:0\n",
      "  %215 : int = prim::Constant[value=1](), scope: AlexNet/Sequential[classifier]/Linear[6] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:1369:0\n",
      "  %216 : Float(1, 1000) = aten::addmm(%bias, %input, %213, %214, %215), scope: AlexNet/Sequential[classifier]/Linear[6] # C:\\ProgramData\\Anaconda3\\envs\\pycon\\lib\\site-packages\\torch\\nn\\functional.py:1369:0\n",
      "  return (%216)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define an exemplary input\n",
    "x = torch.randn(1, 3, 244, 244)\n",
    "# Set the module to evalutation mode\n",
    "net.eval()\n",
    "# Build the graph by tracing the forward function\n",
    "traced_net = jit.trace(net, x)\n",
    "print(traced_net.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Our traced network is now ready to be written to disk. For that we use the save() function of the jit module. It works the same as the conentional torch.save() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Save the traced module to file\n",
    "jit.save(traced_net, 'traced_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# That's it folks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We have saved our network. Now let's head over to a fresh notebook where we can load and test it out. (Open the notebook \"Deployment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Boost Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let us have a look at the runtime of our models. First we will run our untraced network and then the one traced by the JIT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997ms mean with 255ms std for 10 runs with 10 loops each\n"
     ]
    }
   ],
   "source": [
    "# Measure runtime for untraced network\n",
    "untraced_runs = []\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "for _ in range(10):\n",
    "    with torch.autograd.profiler.profile() as profile:\n",
    "        for _ in range(10):\n",
    "            net(x)\n",
    "    untraced_runs.append(profile.self_cpu_time_total / 1000)\n",
    "print('%.0fms mean with %.0fms std for 10 runs with 10 loops each' % (np.mean(untraced_runs), np.std(untraced_runs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "796ms mean with 97ms std for 10 runs with 10 loops each\n"
     ]
    }
   ],
   "source": [
    "# Measure runtime for traced network\n",
    "traced_runs = []\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "for _ in range(10):\n",
    "    with torch.autograd.profiler.profile() as profile:\n",
    "        for _ in range(10):\n",
    "            traced_net(x)\n",
    "    traced_runs.append(profile.self_cpu_time_total / 1000)\n",
    "print('%.0fms mean with %.0fms std for 10 runs with 10 loops each' % (np.mean(traced_runs), np.std(traced_runs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It seems that our traced network is slightly faster. An ANOVA on the runs can tell us if the difference is statistically significant. We get a p-value grater than 0.05, far away from any significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F_onewayResult(statistic=4.917216861649398, pvalue=0.039699343550967055)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate if runtime difference is statistically significant\n",
    "f_oneway(untraced_runs, traced_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "But this was only on the CPU. What about performance on GPU? As we have no GPU available here, we will import the timeit runs from a machine equiped with a GTX 1080Ti. This time the ANOVA shows a p-value of 9e-9, which is by significant by all accounts. Unfortunatelly the traced network is only 1.5 ms faster on average, which is a quite small improvement. For better illustration: the untraced network can process 53 images per second, while the traced can do 58."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F_onewayResult(statistic=99.06722724800497, pvalue=9.598289543000189e-09)\n",
      "Untraced: 19ms +- 0.42ms\n",
      "Traced: 17ms +- 0.03ms\n"
     ]
    }
   ],
   "source": [
    "# Load and compare runtime of AlexNet on GPU\n",
    "with open('torchvision_timings.pkl', mode='rb') as f:\n",
    "    time_dict = pickle.load(f)\n",
    "print(f_oneway(time_dict['alexnet']['untraced'], time_dict['alexnet']['traced']))\n",
    "print('Untraced: %.0fms +- %.2fms' % (np.mean(time_dict['alexnet']['untraced']), np.std(time_dict['alexnet']['untraced'])))\n",
    "print('Traced: %.0fms +- %.2fms' % (np.mean(time_dict['alexnet']['traced']), np.std(time_dict['alexnet']['traced'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The last promise is theoretically fulfilled for this network. No significant speedup is meassurable for our simple network on CPU and a small one on GPU. We will look at some advanced uses of the JIT next and see if our findings hold up there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Performance of TorchVision Networks\n",
    "\n",
    "![Runtime comparisons of Torchvision networks](./torchvision_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For the relatively simple AlexNet we now know what we can expect from tracing it. What about more complex networks? For this we will repeat the process above for each network in torchvision. The JIT will inform us, that the outputs of some traced networks do not correspond with the untraced ones. As we are using untrained versions of the networks, we will ignore this warning for now. Again we will have a look at the results from a machine with a GPU.\n",
    "\n",
    "As we can see, there is next to no difference in mean execution time for all networks. While most networks report significant differences in execution time, the absolute difference is, again, magnitudes smaller than execution time itself. The googlenet architecture alone seems faster when traced. Looking at the source code I could not make out any differences in this network, compared to the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Boost Performance ð"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Promises\n",
    "\n",
    "### 1. Minimize Dependencies â\n",
    "### 2. Hide Code ð\n",
    "### 3. Boost Performance ð"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tips for Migrating Your Model Code\n",
    "\n",
    "* Avoid non-traceable code when possible\n",
    "    * Tracing is easy, scripting is hard\n",
    "    * Most networks are feed forward in inference anyway\n",
    "\n",
    "* Convert module-by-module for debugging\n",
    "    * Only script the needed modules\n",
    "    * Tracing and scripting can be mixed\n",
    "\n",
    "* Include your interface in the graph\n",
    "    * Define an general interface for your deployment code\n",
    "    * Include bridging code in the graph (e.g. input normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## This Talk on GitHub: _github.com/tilman151/pytorch_jit_pycon19_"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
